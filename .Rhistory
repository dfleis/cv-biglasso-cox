# https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring
p <- length(beta)
X <- matrix(rnorm(n * p), nrow = n)
# Weibull latent event times
v <- runif(n = n)
latent.time <- (-log(v)/(lambda * exp(X %*% beta)))^(1/rho)
# censoring times
cens.time <- rexp(n = n, rate = rate.cens)
# follow-up times and event indicators
time <- pmin(latent.time, cens.time)
if (round.times) time <- floor(time * 10^round.digits)/(10^round.digits) + 1#round(time, round.digits)
status <- as.numeric(latent.time < cens.time)
y <- cbind(time, status)
colnames(y) <- c("time", "status")
# data set
return (list(X = X, y = y))
}
sim.surv <- function(n, p, p_nz, seed) {
if (!missing(seed)) set.seed(seed)
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta)
list("beta" = beta, "y" = dat$y, "X" = dat$X)
}
#=======================================#
#================ SETUP ================#
#=======================================#
#remove.packages("biglasso")
#devtools::install_github("dfleis/biglasso")
library(biglasso)
library(glmnet)
library(data.table) # fread() so I can read fewer columns while testing, otherwise read.csv is fine
options(datatable.fread.datatable=FALSE) # format the data as a data.frame instead of a data.table
set.seed(124)
n <- 100
p <- 500
p_nz <- 0.1
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta, rho = 10, rate.cens = 0.05, round.times=T)
y <- dat$y
X <- dat$X
Xbig <- as.big.matrix(X)
table(beta != 0)
table(y[,1], y[,2])
table(y[,2])
#===========================================#
#================ RUN TESTS ================#
#===========================================#
# load custom R functions (mainly plot tools for the following tests)
{lapply(list.files("./R/", full.names = T), source); invisible()}
set.seed(124) # necessary for testing as the bigmemory package has an (unintended?) effect on random number generation
penalty  <- "enet"
alpha    <- 0.5 # elastic net penalty (alpha = 0 is ridge and alpha = 1 is lasso)
nfolds   <- 10
lambda   <- exp(seq(0, -6, length.out = 100))
grouped  <- T # grouped = F is 'basic' CV loss, grouped = T is 'V&VH' CV loss. see https://arxiv.org/pdf/1905.10432.pdf
parallel <- T # "use parallel  to fit each fold. Must register parallel before hand, such as doMC or others"
ncores   <- 10
trace.it <- 1
foldid   <- sample(cut(1:nrow(y), breaks = nfolds, labels = F))
pt <- proc.time()
cv.bl <- cv.biglasso(X       = Xbig,
y       = y,
family  = "cox",
penalty = penalty,
alpha   = alpha,
nfolds  = nfolds,
lambda  = lambda,
grouped = grouped,
cv.ind  = foldid,
ncores  = ncores,
trace   = as.logical(trace.it))
tm.bl <- proc.time() - pt
if (parallel) {doMC::registerDoMC(cores = ncores)}
pt <- proc.time()
cv.gn <- cv.glmnet(x        = X,
y        = y,
family   = "cox",
alpha    = alpha,
nfolds   = nfolds,
lambda   = lambda,
grouped  = grouped,
parallel = parallel,
trace.it = trace.it,
foldid   = foldid)
tm.gn <- proc.time() - pt
#===========================================#
#================= FIGURES =================#
#===========================================#
###
plot(cv.bl)
plot(cv.gn)
### PLOT 1
plot.compare.cv2(cv.bl, cv.gn)
beta.bl <- cv.bl$fit$beta
beta.gn <- cv.gn$glmnet.fit$beta
beta.bl.cv <- cv.bl$fit$beta[,which(cv.bl$lambda == cv.bl$lambda.min)]
beta.gn.cv <- cv.gn$glmnet.fit$beta[,which(cv.gn$lambda == cv.gn$lambda.min)]
### PLOT 2
myclrs <- c(rgb(26/255, 133/255, 255/255, 1),
rgb(212/255, 17/255, 89/255, 0.6))
plot(NA, xlim = c(1, ncol(X)), ylim = range(beta.bl.cv, beta.gn.cv),
ylab = "Coef. Value", xlab = "Covariate Index")
grid(); abline(h = 0, lwd = 2, col = 'gray50'); abline(v = ncol(X1) + 0.5)
for (i in 1:ncol(X)) {
#if (beta[i] != 0) points(x = i, y = beta[i], pch = 19, cex = 0.5, type = 'h')
if (beta.bl.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.bl.cv[i], lwd = 4, col = myclrs[1])
if (beta.gn.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.gn.cv[i], lwd = 4, col = myclrs[2])
}
legend("topleft", legend = c("biglasso", "glmnet"), col = myclrs, seg.len = 2, lwd = 4)
# plot(cv.bl$fit)
# plot(cv.gn$glmnet.fit)
### PLOT 3
myclrs2 <- c(rgb(26/255, 133/255, 255/255, 0.5),
rgb(212/255, 17/255, 89/255, 0.5))
plot(NA, xlab = expression(log(lambda)), ylab = "Fitted Coef.",
xlim = range(log(lambda)), ylim = range(as.matrix(beta.gn), as.matrix(beta.bl)),
main = "Elastic-Net Coefficient Paths")
grid(); abline(h = 0, lwd = 2, col = 'gray50')
for (i in 1:ncol(X)) {
lines(beta.bl[i,] ~ log(lambda), col = myclrs2[1])
lines(beta.gn[i,] ~ log(lambda), col = myclrs2[2])
}
legend("topright", legend = c("biglasso", "glmnet"), col = myclrs2, seg.len = 2, lwd = 4)
tm.bl
tm.gn
###############################################################
# SIMULATED DATA
#
# Comparing glmnet & the (proposed) biglasso CV outputs
# for penalized Cox models
#
###############################################################
#=======================================#
#====== DATA GENERATING MECHANISM ======#
#=======================================#
sim.surv.weib <- function(n, lambda=0.01, rho=1, beta, rate.cens=0.001, round.times=F, round.digits=0) {
# generate survival data (Weibull baseline hazard), adapted from
# https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring
p <- length(beta)
X <- matrix(rnorm(n * p), nrow = n)
# Weibull latent event times
v <- runif(n = n)
latent.time <- (-log(v)/(lambda * exp(X %*% beta)))^(1/rho)
# censoring times
cens.time <- rexp(n = n, rate = rate.cens)
# follow-up times and event indicators
time <- pmin(latent.time, cens.time)
if (round.times) time <- floor(time * 10^round.digits)/(10^round.digits) + 1#round(time, round.digits)
status <- as.numeric(latent.time < cens.time)
y <- cbind(time, status)
colnames(y) <- c("time", "status")
# data set
return (list(X = X, y = y))
}
sim.surv <- function(n, p, p_nz, seed) {
if (!missing(seed)) set.seed(seed)
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta)
list("beta" = beta, "y" = dat$y, "X" = dat$X)
}
#=======================================#
#================ SETUP ================#
#=======================================#
#remove.packages("biglasso")
#devtools::install_github("dfleis/biglasso")
library(biglasso)
library(glmnet)
library(data.table) # fread() so I can read fewer columns while testing, otherwise read.csv is fine
options(datatable.fread.datatable=FALSE) # format the data as a data.frame instead of a data.table
set.seed(124)
n <- 100
p <- 500
p_nz <- 0.1
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta, rho = 10, rate.cens = 0.05, round.times=T)
y <- dat$y
X <- dat$X
Xbig <- as.big.matrix(X)
table(beta != 0)
table(y[,1], y[,2])
table(y[,2])
#===========================================#
#================ RUN TESTS ================#
#===========================================#
# load custom R functions (mainly plot tools for the following tests)
{lapply(list.files("./R/", full.names = T), source); invisible()}
set.seed(124) # necessary for testing as the bigmemory package has an (unintended?) effect on random number generation
penalty  <- "enet"
alpha    <- 0.5 # elastic net penalty (alpha = 0 is ridge and alpha = 1 is lasso)
nfolds   <- 10
lambda   <- exp(seq(0, -6, length.out = 100))
grouped  <- T # grouped = F is 'basic' CV loss, grouped = T is 'V&VH' CV loss. see https://arxiv.org/pdf/1905.10432.pdf
parallel <- T # "use parallel  to fit each fold. Must register parallel before hand, such as doMC or others"
ncores   <- 10
trace.it <- 1
foldid   <- sample(cut(1:nrow(y), breaks = nfolds, labels = F))
pt <- proc.time()
cv.bl <- cv.biglasso(X       = Xbig,
y       = y,
family  = "cox",
penalty = penalty,
alpha   = alpha,
nfolds  = nfolds,
lambda  = lambda,
grouped = grouped,
cv.ind  = foldid,
ncores  = ncores,
trace   = as.logical(trace.it))
tm.bl <- proc.time() - pt
if (parallel) {doMC::registerDoMC(cores = ncores)}
pt <- proc.time()
cv.gn <- cv.glmnet(x        = X,
y        = y,
family   = "cox",
alpha    = alpha,
nfolds   = nfolds,
lambda   = lambda,
grouped  = grouped,
parallel = parallel,
trace.it = trace.it,
foldid   = foldid)
tm.gn <- proc.time() - pt
###
plot(cv.bl)
plot(cv.gn)
### PLOT 1
plot.compare.cv2(cv.bl, cv.gn)
beta.bl <- cv.bl$fit$beta
beta.gn <- cv.gn$glmnet.fit$beta
beta.bl.cv <- cv.bl$fit$beta[,which(cv.bl$lambda == cv.bl$lambda.min)]
beta.gn.cv <- cv.gn$glmnet.fit$beta[,which(cv.gn$lambda == cv.gn$lambda.min)]
### PLOT 2
myclrs <- c(rgb(26/255, 133/255, 255/255, 1),
rgb(212/255, 17/255, 89/255, 0.6))
plot(NA, xlim = c(1, ncol(X)), ylim = range(beta.bl.cv, beta.gn.cv),
ylab = "Coef. Value", xlab = "Covariate Index")
grid(); abline(h = 0, lwd = 2, col = 'gray50'); abline(v = ncol(X1) + 0.5)
for (i in 1:ncol(X)) {
#if (beta[i] != 0) points(x = i, y = beta[i], pch = 19, cex = 0.5, type = 'h')
if (beta.bl.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.bl.cv[i], lwd = 4, col = myclrs[1])
if (beta.gn.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.gn.cv[i], lwd = 4, col = myclrs[2])
}
legend("topleft", legend = c("biglasso", "glmnet"), col = myclrs, seg.len = 2, lwd = 4)
### PLOT 3
myclrs2 <- c(rgb(26/255, 133/255, 255/255, 0.5),
rgb(212/255, 17/255, 89/255, 0.5))
### PLOT 2
myclrs <- c(rgb(26/255, 133/255, 255/255, 1),
rgb(212/255, 17/255, 89/255, 0.6))
plot(NA, xlim = c(1, ncol(X)), ylim = range(beta.bl.cv, beta.gn.cv),
ylab = "Coef. Value", xlab = "Covariate Index")
grid(); abline(h = 0, lwd = 2, col = 'gray50')
for (i in 1:ncol(X)) {
#if (beta[i] != 0) points(x = i, y = beta[i], pch = 19, cex = 0.5, type = 'h')
if (beta.bl.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.bl.cv[i], lwd = 4, col = myclrs[1])
if (beta.gn.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.gn.cv[i], lwd = 4, col = myclrs[2])
}
legend("topleft", legend = c("biglasso", "glmnet"), col = myclrs, seg.len = 2, lwd = 4)
### PLOT 3
myclrs2 <- c(rgb(26/255, 133/255, 255/255, 0.5),
rgb(212/255, 17/255, 89/255, 0.5))
plot(NA, xlab = expression(log(lambda)), ylab = "Fitted Coef.",
xlim = range(log(lambda)), ylim = range(as.matrix(beta.gn), as.matrix(beta.bl)),
main = "Elastic-Net Coefficient Paths")
grid(); abline(h = 0, lwd = 2, col = 'gray50')
for (i in 1:ncol(X)) {
lines(beta.bl[i,] ~ log(lambda), col = myclrs2[1])
lines(beta.gn[i,] ~ log(lambda), col = myclrs2[2])
}
legend("topright", legend = c("biglasso", "glmnet"), col = myclrs2, seg.len = 2, lwd = 4)
###############################################################
# SIMULATED DATA
#
# Comparing glmnet & the (proposed) biglasso CV outputs
# for penalized Cox models
#
###############################################################
#=======================================#
#====== DATA GENERATING MECHANISM ======#
#=======================================#
sim.surv.weib <- function(n, lambda=0.01, rho=1, beta, rate.cens=0.001, round.times=F, round.digits=0) {
# generate survival data (Weibull baseline hazard), adapted from
# https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring
p <- length(beta)
X <- matrix(rnorm(n * p), nrow = n)
# Weibull latent event times
v <- runif(n = n)
latent.time <- (-log(v)/(lambda * exp(X %*% beta)))^(1/rho)
# censoring times
cens.time <- rexp(n = n, rate = rate.cens)
# follow-up times and event indicators
time <- pmin(latent.time, cens.time)
if (round.times) time <- floor(time * 10^round.digits)/(10^round.digits) + 1#round(time, round.digits)
status <- as.numeric(latent.time < cens.time)
y <- cbind(time, status)
colnames(y) <- c("time", "status")
# data set
return (list(X = X, y = y))
}
sim.surv <- function(n, p, p_nz, seed) {
if (!missing(seed)) set.seed(seed)
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta)
list("beta" = beta, "y" = dat$y, "X" = dat$X)
}
#=======================================#
#================ SETUP ================#
#=======================================#
#remove.packages("biglasso")
#devtools::install_github("dfleis/biglasso")
library(biglasso)
library(glmnet)
library(data.table) # fread() so I can read fewer columns while testing, otherwise read.csv is fine
options(datatable.fread.datatable=FALSE) # format the data as a data.frame instead of a data.table
set.seed(124)
n <- 100
p <- 500
p_nz <- 0.1
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta, rho = 10, rate.cens = 0.05, round.times=T)
y <- dat$y
X <- dat$X
Xbig <- as.big.matrix(X)
table(beta != 0)
table(y[,1], y[,2])
table(y[,2])
#===========================================#
#================ RUN TESTS ================#
#===========================================#
# load custom R functions (mainly plot tools for the following tests)
{lapply(list.files("./R/", full.names = T), source); invisible()}
set.seed(124) # necessary for testing as the bigmemory package has an (unintended?) effect on random number generation
penalty  <- "enet"
alpha    <- 0.5 # elastic net penalty (alpha = 0 is ridge and alpha = 1 is lasso)
nfolds   <- 10
lambda   <- exp(seq(0, -6, length.out = 100))
grouped  <- F # grouped = F is 'basic' CV loss, grouped = T is 'V&VH' CV loss. see https://arxiv.org/pdf/1905.10432.pdf
parallel <- T # "use parallel  to fit each fold. Must register parallel before hand, such as doMC or others"
ncores   <- 10
trace.it <- 1
foldid   <- sample(cut(1:nrow(y), breaks = nfolds, labels = F))
pt <- proc.time()
cv.bl <- cv.biglasso(X       = Xbig,
y       = y,
family  = "cox",
penalty = penalty,
alpha   = alpha,
nfolds  = nfolds,
lambda  = lambda,
grouped = grouped,
cv.ind  = foldid,
ncores  = ncores,
trace   = as.logical(trace.it))
tm.bl <- proc.time() - pt
if (parallel) {doMC::registerDoMC(cores = ncores)}
pt <- proc.time()
cv.gn <- cv.glmnet(x        = X,
y        = y,
family   = "cox",
alpha    = alpha,
nfolds   = nfolds,
lambda   = lambda,
grouped  = grouped,
parallel = parallel,
trace.it = trace.it,
foldid   = foldid)
tm.gn <- proc.time() - pt
#===========================================#
#================= FIGURES =================#
#===========================================#
###
plot(cv.bl)
plot(cv.gn)
### PLOT 1
plot.compare.cv2(cv.bl, cv.gn)
beta.bl <- cv.bl$fit$beta
beta.gn <- cv.gn$glmnet.fit$beta
beta.bl.cv <- cv.bl$fit$beta[,which(cv.bl$lambda == cv.bl$lambda.min)]
beta.gn.cv <- cv.gn$glmnet.fit$beta[,which(cv.gn$lambda == cv.gn$lambda.min)]
### PLOT 2
myclrs <- c(rgb(26/255, 133/255, 255/255, 1),
rgb(212/255, 17/255, 89/255, 0.6))
plot(NA, xlim = c(1, ncol(X)), ylim = range(beta.bl.cv, beta.gn.cv),
ylab = "Coef. Value", xlab = "Covariate Index")
grid(); abline(h = 0, lwd = 2, col = 'gray50')
for (i in 1:ncol(X)) {
#if (beta[i] != 0) points(x = i, y = beta[i], pch = 19, cex = 0.5, type = 'h')
if (beta.bl.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.bl.cv[i], lwd = 4, col = myclrs[1])
if (beta.gn.cv[i] != 0) segments(x0 = i, x1 = i, y0 = 0, y1 = beta.gn.cv[i], lwd = 4, col = myclrs[2])
}
legend("topleft", legend = c("biglasso", "glmnet"), col = myclrs, seg.len = 2, lwd = 4)
# plot(cv.bl$fit)
# plot(cv.gn$glmnet.fit)
### PLOT 3
myclrs2 <- c(rgb(26/255, 133/255, 255/255, 0.5),
rgb(212/255, 17/255, 89/255, 0.5))
plot(NA, xlab = expression(log(lambda)), ylab = "Fitted Coef.",
xlim = range(log(lambda)), ylim = range(as.matrix(beta.gn), as.matrix(beta.bl)),
main = "Elastic-Net Coefficient Paths")
grid(); abline(h = 0, lwd = 2, col = 'gray50')
for (i in 1:ncol(X)) {
lines(beta.bl[i,] ~ log(lambda), col = myclrs2[1])
lines(beta.gn[i,] ~ log(lambda), col = myclrs2[2])
}
legend("topright", legend = c("biglasso", "glmnet"), col = myclrs2, seg.len = 2, lwd = 4)
tm.bl
tm.gn
###############################################################
# SIMULATED DATA
#
# Comparing glmnet & the (proposed) biglasso CV outputs
# for penalized Cox models
#
###############################################################
#=======================================#
#====== DATA GENERATING MECHANISM ======#
#=======================================#
sim.surv.weib <- function(n, lambda=0.01, rho=1, beta, rate.cens=0.001, round.times=F, round.digits=0) {
# generate survival data (Weibull baseline hazard), adapted from
# https://stats.stackexchange.com/questions/135124/how-to-create-a-toy-survival-time-to-event-data-with-right-censoring
p <- length(beta)
X <- matrix(rnorm(n * p), nrow = n)
# Weibull latent event times
v <- runif(n = n)
latent.time <- (-log(v)/(lambda * exp(X %*% beta)))^(1/rho)
# censoring times
cens.time <- rexp(n = n, rate = rate.cens)
# follow-up times and event indicators
time <- pmin(latent.time, cens.time)
if (round.times) time <- floor(time * 10^round.digits)/(10^round.digits) + 1#round(time, round.digits)
status <- as.numeric(latent.time < cens.time)
y <- cbind(time, status)
colnames(y) <- c("time", "status")
# data set
return (list(X = X, y = y))
}
sim.surv <- function(n, p, p_nz, seed) {
if (!missing(seed)) set.seed(seed)
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta)
list("beta" = beta, "y" = dat$y, "X" = dat$X)
}
#=======================================#
#================ SETUP ================#
#=======================================#
#remove.packages("biglasso")
#devtools::install_github("dfleis/biglasso")
library(biglasso)
library(glmnet)
library(data.table) # fread() so I can read fewer columns while testing, otherwise read.csv is fine
options(datatable.fread.datatable=FALSE) # format the data as a data.frame instead of a data.table
set.seed(124)
n <- 100
p <- 500
p_nz <- 0.1
beta <- rnorm(p, 0, 1) * rbinom(p, 1, p_nz)
dat <- sim.surv.weib(n = n, beta = beta, rho = 10, rate.cens = 0.05, round.times=T)
y <- dat$y
X <- dat$X
Xbig <- as.big.matrix(X)
table(beta != 0)
table(y[,1], y[,2])
table(y[,2])
#===========================================#
#================ RUN TESTS ================#
#===========================================#
# load custom R functions (mainly plot tools for the following tests)
{lapply(list.files("./R/", full.names = T), source); invisible()}
set.seed(124) # necessary for testing as the bigmemory package has an (unintended?) effect on random number generation
penalty  <- "enet"
alpha    <- 0.5 # elastic net penalty (alpha = 0 is ridge and alpha = 1 is lasso)
nfolds   <- 10
lambda   <- exp(seq(0, -6, length.out = 100))
grouped  <- T # grouped = F is 'basic' CV loss, grouped = T is 'V&VH' CV loss. see https://arxiv.org/pdf/1905.10432.pdf
parallel <- T # "use parallel  to fit each fold. Must register parallel before hand, such as doMC or others"
ncores   <- 10
trace.it <- 1
foldid   <- sample(cut(1:nrow(y), breaks = nfolds, labels = F))
pt <- proc.time()
cv.bl <- cv.biglasso(X       = Xbig,
y       = y,
family  = "cox",
penalty = penalty,
alpha   = alpha,
nfolds  = nfolds,
lambda  = lambda,
grouped = grouped,
cv.ind  = foldid,
ncores  = ncores,
trace   = as.logical(trace.it))
tm.bl <- proc.time() - pt
if (parallel) {doMC::registerDoMC(cores = ncores)}
pt <- proc.time()
cv.gn <- cv.glmnet(x        = X,
y        = y,
family   = "cox",
alpha    = alpha,
nfolds   = nfolds,
lambda   = lambda,
grouped  = grouped,
parallel = parallel,
trace.it = trace.it,
foldid   = foldid)
tm.gn <- proc.time() - pt
#===========================================#
#================= FIGURES =================#
#===========================================#
###
plot(cv.bl)
plot(cv.gn)
### PLOT 1
plot.compare.cv2(cv.bl, cv.gn)
